try:
    import pandas as pd
    import numpy as np
    from collections import defaultdict, OrderedDict
    import operator
    from itertools import islice
    import os
    import csv
    import math
    import sys
    from math import log
    from datetime import datetime
    from sklearn.neural_network import MLPClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import cross_val_score
    from sklearn.model_selection import ShuffleSplit
    import matplotlib.pyplot as plt
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import learning_curve
    from sklearn.model_selection import GridSearchCV
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import precision_recall_curve
    from sklearn.metrics import average_precision_score
    from sklearn.metrics import roc_curve, auc
    from itertools import cycle
except ImportError as e:
    print("{} cannot be imported".format(e))


class Exploit_Algorithm(object):
    
    def __init__(self, data_file, number_of_customers):
        self.data = pd.read_csv(data_file)
        self.customer = list(set(self.data['customerkey']))[:number_of_customers]
        self.data = self.data[self.data['customerkey'].isin(self.customer)]
        self.all_train = []
        self.all_labels = []
        self.Main()

    def Main(self):
        self.data.columns = ['Date', 'customerkey', 'trans_nums', 'sku', 'promo']

        def remove_new(list_one, list_two):

            first_func = lambda x: x[0]
            common = lambda x, y: set(x).intersection(set(y))
            first_one = list(map(first_func, list_one))
            first_rwo = list(map(first_func, list_two))
            commons = list(common(first_one, first_rwo))
            return [vals for vals in list_one if vals[0] in commons]

        for customers in self.customer:
            data_customer = self.data[self.data['customerkey'] == customers]
            train_generator = self.data_processing(data_customer, train = True)
            test_generator = self.data_processing(data_customer)
            feature_train = self.Feature_Creation(train_generator[0], train_generator[1])
            feature_test = self.Feature_Creation(test_generator[0], test_generator[1])
            result_train = feature_train.consolidation(train = True)
            result_label = feature_test.consolidation()

            self.all_train += result_train
            self.all_labels += remove_new(result_label, result_train)
        self.all_train = [tup[1] for tup in self.all_train]
        self.all_labels = [tups[1] for tups in self.all_labels]


        DeepLearning = self.Multi_Layer_Perceptron(self.all_train, self.all_labels)
        DeepLearning.Model_Selection()


        print("CORRECT")

    def data_processing(self, rec_data, train = None):
        # Choosing a customer to test the engine

        all_dates = sorted(list(set(rec_data['Date'])))

        def df_to_nested_lists(df):
            groupBY = df.groupby(['Date', 'customerkey'])['sku']
            # Get the index : product
            dic = df['sku'].to_dict()
            # Get the values from the groups as a list
            bas_index = list(groupBY.groups.values())
            # Nested list comprehension to replace the index with the product
            baskets = [[dic[x] for x in line] for line in bas_index]
            print("The baskets are done")
            return baskets
        if train:
            train_customer = rec_data[rec_data['Date'] <= all_dates[-2::][0]]
            return train_customer, df_to_nested_lists(train_customer)
        else:
            test_customer = rec_data[rec_data['Date'] <= all_dates[-1::][0]]
            return test_customer, df_to_nested_lists(test_customer)

    class Feature_Creation(object):

        def __init__(self, data, corpus):
            self.data = data
            self.corpus = corpus
            self.result = defaultdict(list)

        def tf(self):
            tf_dic  = defaultdict(int)
            for doc in self.corpus:
                for word in doc:
                    tf_dic[word] += 1
            for k, v in tf_dic.items():
                self.result[k].append(v)
            del tf_dic
            print("The tf feature is calculated")
            return self

        def idf(self):
            self.tf()
            for docs in self.result.keys():
                self.result[docs].append(math. \
                log(len(self.corpus)/self.result[docs][0], 2))
            print("The idf feature is calculated")
            return self

        def promotion_price_mean(self):
            self.idf()
            new_promo = self.data[['sku', 'promo']]
            new = pd.DataFrame({'mean': new_promo.groupby('sku')['promo']
                               .mean()}).reset_index().values
            del new_promo
            for lines in new:
                self.result[lines[0]].append(lines[1])
            print("The promotion_price_mean is calculated")
            return self

        def relevance(self):
            self.promotion_price_mean()
            gamma = 0.9
            ordered_dates = sorted(list(set(self.data['Date'])), reverse = True)
            length = len(ordered_dates)
            def gamma_list(gamma, length):
                return sorted([(gamma ** i, j)
                               for i, j in zip(range(1, length + 1),
                                ordered_dates)], reverse = True)

            gamma_ls = pd.DataFrame(gamma_list(gamma, length),
                                    columns=['relevance', 'Date'])

            sku_date = self.data[['sku', 'Date']]
            sku_date = sku_date.\
                sort_values(by = ['Date'], ascending= False)
            sku_date_max = sku_date.groupby('sku', as_index = False).first()
            sku_date_max_rel = pd.merge(sku_date_max, gamma_ls,
                                        left_on='Date',right_on='Date', how='left')
            sku_date_max_rel = sku_date_max_rel[['sku', 'relevance']]
            rel_dict = list(sku_date_max_rel.set_index('sku')
                            .to_dict()
                            .values())[0]
            for k, v in rel_dict.items():
                self.result[k].append(v)
            print("The relevance feature is calculated")
            return self

        def time_feature(self):
            self.relevance()
            def days_between(d1, d2):
                d1 = datetime.strptime(d1, "%Y-%m-%d")
                d2 = datetime.strptime(d2, "%Y-%m-%d")
                return abs((d2 - d1).days)
            self.data['freq'] = self.data.groupby('sku')['sku'].transform('count')
            self.data = self.data.sort_values(by = ['sku','Date'],
                                              ascending = [True, True])

            iter_data = self.data[self.data['freq'] > 1]
            no_iter_data = self.data[self.data['freq'] == 1]
            array =  iter_data[['Date','sku']].values
            time_value = 0
            i = 0
            for ind in range(len(array) - 1):
                if array[ind][1] == array[ind+1][1]:
                    i += 1
                    time_value += days_between(array[ind][0], array[ind + 1][0])
                    if ind + 1 == len(array) - 1:
                        self.result[array[ind][1]].append(time_value / i + 1)
                        break
                else:
                    self.result[array[ind][1]].append(time_value / i + 1)
                    time_value, i = 0, 0


            for skus_one in no_iter_data.sku.unique():
                self.result[skus_one].\
                    append(days_between(max(self.data['Date']),
                                        min(self.data['Date'])))
            print("The time feature is finished")
            return self


        def consolidation(self, train = None, objective_func = None):


            def logistic(x):
                return np.divide(1, 1 + math.exp(-x))

            def Label(row):
                if row['rank'] <= 20:
                    return 1
                else:
                    return 0

            def Label1(row):
                if row['Date'] == max_date:
                    return 1
                else:
                    return 0

            def Calculation(row):
                return np.divide((row['features'][0]
                                  * row['features'][1]
                                  * row['features'][3])
                                  + row['features'][2], row['features'][4])

            if train and objective_func == True:
                data_features = pd.DataFrame(list(self.result.items()), columns=['sku', 'features'])
                try:
                    data_features['value'] = data_features.apply(Calculation, 1)
                except IndexError:
                    print("IndexError")
                    pass
                data_features['rank'] = data_features['value']\
                    .rank(ascending=0)\
                    .round()
                data_features = data_features.sort_values(by='rank', ascending=True)
                data_features['label'] = data_features.apply(Label, axis=1)
                print("train evaluated")
                return data_features[['label','features']]
            elif train == True:
                self.time_feature()
                sorted_t = sorted(self.result.items(), key = operator.itemgetter(0))
                return sorted_t

            else:
                max_date = max(list(set(self.data['Date'])))
                label_data = self.data
                label_data['label'] = label_data.apply(Label1, 1)
                label_data = label_data[['sku', 'label']].values
                label_dic = {line[0]:line[1] for line in label_data}
                sorted_l = sorted(label_dic.items(), key = operator.itemgetter(0))
                return sorted_l

    class Multi_Layer_Perceptron(object):

        def __init__(self,
                     train_x,
                     train_y):
            self.train_x = train_x
            self.train_y = train_y

        def Model_Selection(self):

            cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0)

            pipeline = Pipeline([
                ('scal', StandardScaler()),
                ('mlp', MLPClassifier())])

            parameters = {'mlp__solver': ('lbfgs', 'adam'),
                          'mlp__activation': ('logistic', 'relu'),
                          'mlp__alpha': [0.1, 0.01],
                          'mlp__hidden_layer_sizes': [(100,), (150,)]}

            estimator = GridSearchCV(estimator=pipeline,
                                     param_grid=parameters,
                                     scoring='f1_weighted',
                                     cv=cv,
                                     verbose=1,
                                     n_jobs=1)

            print("Performing grid search...")
            print("pipeline:", [name for name, _ in pipeline.steps])
            print("parameters:")
            print(parameters)

            estimator.fit(self.train_x, self.train_y)
            print()

            print("Best score: %0.3f" % estimator.best_score_)
            print("Best parameters set:")
            best_parameters = estimator.best_estimator_.get_params()
            for param_name in sorted(parameters.keys()):
                print("\t%s: %r" % (param_name, best_parameters[param_name]))
            print("The pipeline is set")

        def Training(self):
            pass

        def plot_learning_curve(self, estimator, title, X, y, ylim=None, cv=None,
                                n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):

            plt.figure()
            plt.interactive(False)
            plt.title(title)
            if ylim is not None:
                plt.ylim(*ylim)
            plt.xlabel("Training examples")
            plt.ylabel("Score")
            train_sizes, train_scores, test_scores = learning_curve(
                estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
            train_scores_mean = np.mean(train_scores, axis=1)
            train_scores_std = np.std(train_scores, axis=1)
            test_scores_mean = np.mean(test_scores, axis=1)
            test_scores_std = np.std(test_scores, axis=1)
            plt.grid()

            plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                             train_scores_mean + train_scores_std, alpha=0.1,
                             color="r")
            plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                             test_scores_mean + test_scores_std, alpha=0.1, color="g")
            plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
                     label="Training score")
            plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
                     label="Cross-validation score")

            plt.legend(loc="best")
            print("Returns")
            return plt










